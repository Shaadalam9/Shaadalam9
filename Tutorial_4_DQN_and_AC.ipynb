{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaadalam9/Shaadalam9/blob/main/Tutorial_4_DQN_and_AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7PKu9r0asK"
      },
      "source": [
        "#**Tutorial 4 - DQN and Actor-Critic**\n",
        "\n",
        "Please follow this tutorial to understand the structure (code) of DQNs & get familiar with Actor Critic methods.\n",
        "\n",
        "\n",
        "### References:\n",
        "\n",
        "Please follow [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) for the original publication as well as the psuedocode. Watch Prof. Ravi's lectures on moodle or nptel for further understanding the core concepts. Contact the TAs for further resources if needed. \n",
        "\n",
        "\n",
        "##**Part 1: DQN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azUjb7UK4Yfh",
        "outputId": "13188744-ac07-45f1-a296-46e6c0f0d1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (60.9.3)\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Installing packages for rendering the game on Colab\n",
        "'''\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfCgocAHOZ6n",
        "outputId": "a82b1b50-7a0a-474e-e20f-9df75c370dcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.10.0.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (60.9.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (13.0.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.21.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.13.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.44.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.24.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.17.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow-gpu) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow-gpu) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow-gpu) (1.8.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow-gpu) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_DODRgW_ZKS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "A bunch of imports, you don't have to worry about these\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNA5kiH_esJ",
        "outputId": "279948e2-8576-4ee2-e518-08b75141ae22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "1\n",
            "----\n",
            "[-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
            "----\n",
            "0\n",
            "----\n",
            "[-0.04363321 -0.14877061  0.01284913  0.2758415 ]\n",
            "1.0\n",
            "False\n",
            "{}\n",
            "----\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Please refer to the first tutorial for more details on the specifics of environments\n",
        "We've only added important commands you might find useful for experiments.\n",
        "'''\n",
        "\n",
        "'''\n",
        "List of example environments\n",
        "(Source - https://gym.openai.com/envs/#classic_control)\n",
        "\n",
        "'Acrobot-v1'\n",
        "'CartPole-v0'\n",
        "'MountainCar-v0'\n",
        "'''\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())\n",
        "print(\"----\")\n",
        "\n",
        "'''\n",
        "# Understanding State, Action, Reward Dynamics\n",
        "\n",
        "The agent decides an action to take depending on the state.\n",
        "\n",
        "The Environment keeps a variable specifically for the current state.\n",
        "- Everytime an action is passed to the environment, it calculates the new state and updates the current state variable.\n",
        "- It returns the new current state and reward for the agent to take the next action\n",
        "\n",
        "'''\n",
        "\n",
        "state = env.reset()   \n",
        "''' This returns the initial state (when environment is reset) '''\n",
        "\n",
        "print(state)\n",
        "print(\"----\")\n",
        "\n",
        "action = env.action_space.sample()  \n",
        "''' We take a random action now '''\n",
        "\n",
        "print(action)\n",
        "print(\"----\")\n",
        "\n",
        "next_state, reward, done, info = env.step(action) \n",
        "''' env.step is used to calculate new state and obtain reward based on old state and action taken  ''' \n",
        "\n",
        "print(next_state)\n",
        "print(reward)\n",
        "print(done)\n",
        "print(info)\n",
        "print(\"----\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apuaOxavDXus"
      },
      "source": [
        "## DQN\n",
        "\n",
        "Using NNs as substitutes isn't something new. It has been tried earlier, but the 'human control' paper really popularised using NNs by providing a few stability ideas (Q-Targets, Experience Replay & Truncation). The 'Deep-Q Network' (DQN) Algorithm can be broken down into having the following components. \n",
        "\n",
        "### Q-Network:\n",
        "The neural network used as a function approximator is defined below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4MRC1p2DZbp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "### Q Network & Some 'hyperparameters'\n",
        "\n",
        "QNetwork1:\n",
        "Input Layer - 4 nodes (State Shape) \\\n",
        "Hidden Layer 1 - 64 nodes \\\n",
        "Hidden Layer 2 - 64 nodes \\\n",
        "Output Layer - 2 nodes (Action Space) \\\n",
        "Optimizer - zero_grad()\n",
        "\n",
        "QNetwork2: Feel free to experiment more\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn  \n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "'''\n",
        "Bunch of Hyper parameters (Which you might have to tune later **wink wink**)\n",
        "'''\n",
        "BUFFER_SIZE = int(1e5) # ''' replay buffer size '''\n",
        "BATCH_SIZE = 64         #''' minibatch size '''\n",
        "GAMMA = 0.99            #''' discount factor '''\n",
        "LR = 5e-4               #''' learning rate '''\n",
        "UPDATE_EVERY = 20       #''' how often to update the network (When Q target is present) '''\n",
        "\n",
        "\n",
        "class QNetwork1(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=64):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(QNetwork1, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmv5c0XoK8GA"
      },
      "source": [
        "### Replay Buffer:\n",
        "\n",
        "This is a 'deque' that helps us store experiences. Recall why we use such a technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh_oghc7Ledh"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)  \n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "    \n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "  \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8VJYkqoLqlO"
      },
      "source": [
        "## Truncation:\n",
        "We add a line (optionally) in the code to truncate the gradient in hopes that it would help with the stability of the learning process.\n",
        "\n",
        "## Tutorial Agent Code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ok_5eQM7OCTj"
      },
      "outputs": [],
      "source": [
        "class TutorialAgent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ''' Epsilon-greedy action selection (Already Present) '''\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "            \n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQFbRCHWQyO"
      },
      "source": [
        "### Here, we present the DQN algorithm code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6A2TdUHWVUN",
        "outputId": "34b0a6fe-cf21-47a5-a910-a65fa9e76397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 38.69\n",
            "Episode 200\tAverage Score: 120.20\n",
            "Episode 300\tAverage Score: 109.18\n",
            "Episode 400\tAverage Score: 23.80\n",
            "Episode 500\tAverage Score: 17.78\n",
            "Episode 600\tAverage Score: 28.24\n",
            "Episode 700\tAverage Score: 9.54\n",
            "Episode 800\tAverage Score: 9.46\n",
            "Episode 900\tAverage Score: 12.21\n",
            "Episode 1000\tAverage Score: 13.34\n",
            "Episode 1100\tAverage Score: 9.39\n",
            "Episode 1200\tAverage Score: 9.45\n",
            "Episode 1300\tAverage Score: 11.25\n",
            "Episode 1400\tAverage Score: 17.29\n",
            "Episode 1500\tAverage Score: 25.37\n",
            "Episode 1600\tAverage Score: 36.72\n",
            "Episode 1700\tAverage Score: 162.36\n",
            "Episode 1800\tAverage Score: 134.34\n",
            "Episode 1900\tAverage Score: 150.11\n",
            "Episode 2000\tAverage Score: 156.37\n",
            "Episode 2100\tAverage Score: 151.05\n",
            "Episode 2200\tAverage Score: 104.28\n",
            "Episode 2300\tAverage Score: 89.45\n",
            "Episode 2400\tAverage Score: 27.99\n",
            "Episode 2500\tAverage Score: 31.51\n",
            "Episode 2600\tAverage Score: 19.00\n",
            "Episode 2700\tAverage Score: 21.25\n",
            "Episode 2800\tAverage Score: 21.46\n",
            "Episode 2900\tAverage Score: 12.86\n",
            "Episode 3000\tAverage Score: 28.47\n",
            "Episode 3100\tAverage Score: 26.31\n",
            "Episode 3200\tAverage Score: 13.80\n",
            "Episode 3300\tAverage Score: 16.91\n",
            "Episode 3400\tAverage Score: 18.99\n",
            "Episode 3500\tAverage Score: 21.39\n",
            "Episode 3600\tAverage Score: 21.53\n",
            "Episode 3700\tAverage Score: 20.99\n",
            "Episode 3800\tAverage Score: 20.82\n",
            "Episode 3900\tAverage Score: 22.50\n",
            "Episode 4000\tAverage Score: 75.94\n",
            "Episode 4076\tAverage Score: 195.08\n",
            "Environment solved in 3976 episodes!\tAverage Score: 195.08\n",
            "0:07:57.446785\n"
          ]
        }
      ],
      "source": [
        "''' Defining DQN Algorithm '''\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "\n",
        "def dqn(n_episodes=10000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores = []                 \n",
        "    ''' list containing scores from each episode '''\n",
        "\n",
        "    scores_window_printing = deque(maxlen=10) \n",
        "    ''' For printing in the graph '''\n",
        "    \n",
        "    scores_window= deque(maxlen=100)  \n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "\n",
        "    eps = eps_start                    \n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "\n",
        "        scores_window.append(score)       \n",
        "        scores_window_printing.append(score)   \n",
        "        ''' save most recent score '''           \n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps) \n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")  \n",
        "        if i_episode % 10 == 0: \n",
        "            scores.append(np.mean(scores_window_printing))        \n",
        "        if i_episode % 100 == 0: \n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=195.0:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "           break\n",
        "    return [np.array(scores),i_episode-100]\n",
        "\n",
        "''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "begin_time = datetime.datetime.now()\n",
        "agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
        "\n",
        "\n",
        "dqn()\n",
        "\n",
        "\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "print(time_taken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL9YMq9yPHLk"
      },
      "source": [
        "### **Task 1a**  \n",
        "Understand the core of the algorithm, follow the flow of data. Identify the exploration strategy used.\n",
        "### **Task 1b**\n",
        "Out of the two exploration strategies discussed in class ($Ïµ$-greedy & Softmax). Implement the strategy that's not used here. \n",
        "### **Task 1c**\n",
        "How fast does the agent 'solve' the environment in terms of the number of episodes?\n",
        "(CartPole-v0 defines \"solving\" as getting average reward of 195.0 over 100 consecutive trials. SOURCE - https://gym.openai.com/envs/CartPole-v0/)\n",
        "\n",
        "How 'well' does the agent learn? (reward plot?) The above two are some 'evaluation metrics' you can use to comment on the performance of an algorithm.\n",
        "\n",
        "Please compare DQN (using $\\epsilon$-greedy) with DQN (using softmax). Think along the lines of 'no. of episodes', 'reward plots', 'compute time', etc. and add a few comments.\n",
        "\n",
        "### **Task 1d (Optional)** \n",
        "\n",
        "Take a look at the official submissions page on [OpenAI gym's CartPole v-0 evaluations](https://gym.openai.com/envs/CartPole-v0/) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RH2Hl1FxTVy"
      },
      "source": [
        "**Task 1a**\n",
        "The exploration strategy used here is the epsilon - greedy method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64pKeBW-x2I2"
      },
      "source": [
        "**Task 1b**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QteFAjcBxNG4"
      },
      "outputs": [],
      "source": [
        "class TutorialAgentSoftmax():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.rg = np.random.RandomState(seed)\n",
        "\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.qnetwork_target = QNetwork1(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "\n",
        "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "        self.t_step = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "        \"\"\" +Q TARGETS PRESENT \"\"\"\n",
        "        ''' Updating the Network every 'UPDATE_EVERY' steps taken '''      \n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ''' Softmax- Exploration '''\n",
        "        from scipy.special import softmax\n",
        "        return self.rg.choice(np.arange(self.action_size), p = softmax(action_values.cpu().data.numpy()[0]))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "\n",
        "        ''' Get expected Q values from local model '''\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "            \n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH-tyfBP17Mu",
        "outputId": "cef34166-5531-4778-b588-cd4f3a613a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100\tAverage Score: 41.97\n",
            "Episode 200\tAverage Score: 130.25\n",
            "Episode 300\tAverage Score: 44.07\n",
            "Episode 400\tAverage Score: 62.22\n",
            "Episode 500\tAverage Score: 13.88\n",
            "Episode 600\tAverage Score: 12.26\n",
            "Episode 700\tAverage Score: 192.45\n",
            "Episode 703\tAverage Score: 196.26\n",
            "Environment solved in 603 episodes!\tAverage Score: 196.26\n",
            "0:01:55.315846\n"
          ]
        }
      ],
      "source": [
        "begin_time = datetime.datetime.now()\n",
        "agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
        "\n",
        "\n",
        "dqn()\n",
        "\n",
        "\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "print(time_taken)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1c**"
      ],
      "metadata": {
        "id": "gpOIYTZPpKN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax exploration is converging in less time as well as  in less no. of episodes."
      ],
      "metadata": {
        "id": "TPmhryFqpPIw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LBh6_lOVBdN"
      },
      "source": [
        "#### **Submission Steps**\n",
        "\n",
        "#### Task 1: Add a text cell with the answer.\n",
        "\n",
        "#### Task 2: Add a code cell below task 1 solution and use 'Tutorial Agent Code' to build your new agent (with a different exploration strategy).\n",
        "\n",
        "#### Task 3: Add a code cell below task 2 solution running both the agents to solve the CartPole v-0 environment and add a new text cell below it with your inferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt6xX0njbJvS"
      },
      "source": [
        "## **Part 2: One-Step Actor-Critic Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9WWiiNxhM7x"
      },
      "source": [
        "**Actor-Critic methods** learn both a policy $\\pi(a|s;\\theta)$ and a state-value function $v(s;w)$ simultaneously. The policy is referred to as the actor that suggests actions given a state. The estimated value function is referred to as the critic. It evaluates actions taken by the actor based on the given policy. In this exercise, both functions are approximated by feedforward neural networks. \n",
        "\n",
        "- The policy network is parametrized by $\\theta$ - it takes a state $s$ as input and outputs the probabilities $\\pi(a|s;\\theta)\\ \\forall\\ a$\n",
        "- The value network is parametrized by $w$ - it takes a state $s$ as input and outputs a scalar value associated with the state, i.e., $v(s;w)$\n",
        "- The single step TD error can be defined as follows:\n",
        "$$\\delta_t  = R_{t+1} + \\gamma v(s_{t+1};w) - v(s_t;w)$$\n",
        "- The loss function to be minimized at every step ($L_{tot}^{(t)}$) is a summation of two terms, as follows:\n",
        "$$L_{tot}^{(t)} = L_{actor}^{(t)} + L_{critic}^{(t)}$$\n",
        "where,\n",
        "$$L_{actor}^{(t)} = -\\log\\pi(a_t|s_t; \\theta)\\delta_t$$\n",
        "$$L_{critic}^{(t)} = \\delta_t^2$$\n",
        "- **NOTE: Here, weights of the first two hidden layers are shared by the policy and the value network**\n",
        "    - First two hidden layer sizes: [1024, 512]\n",
        "    - Output size of policy network: 2 (Softmax activation)\n",
        "    - Output size of value network: 1 (Linear activation)\n",
        "\n",
        "<!-- $$\\pi(a|s;\\theta) = \\phi_{\\theta}(a,s)$$ -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eU3_IeYwN3H"
      },
      "source": [
        "### Initializing Actor-Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXFHQcJjVKYu"
      },
      "outputs": [],
      "source": [
        "class ActorCriticModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Defining policy and value networkss\n",
        "    \"\"\"\n",
        "    def __init__(self, action_size, n_hidden1=1024, n_hidden2=512):\n",
        "        super(ActorCriticModel, self).__init__()\n",
        "\n",
        "        #Hidden Layer 1\n",
        "        self.fc1 = tf.keras.layers.Dense(n_hidden1, activation='relu')\n",
        "        #Hidden Layer 2\n",
        "        self.fc2 = tf.keras.layers.Dense(n_hidden2, activation='relu')\n",
        "        \n",
        "        #Output Layer for policy\n",
        "        self.pi_out = tf.keras.layers.Dense(action_size, activation='softmax')\n",
        "        #Output Layer for state-value\n",
        "        self.v_out = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, state):\n",
        "        \"\"\"\n",
        "        Computes policy distribution and state-value for a given state\n",
        "        \"\"\"\n",
        "        layer1 = self.fc1(state)\n",
        "        layer2 = self.fc2(layer1)\n",
        "\n",
        "        pi = self.pi_out(layer2)\n",
        "        v = self.v_out(layer2)\n",
        "\n",
        "        return pi, v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40uV1hrewVnA"
      },
      "source": [
        "### Agent Class\n",
        "###**Task 2a:** Write code to compute $\\delta_t$ inside the Agent.learn() function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RNpp9WMfkTE"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent class\n",
        "    \"\"\"\n",
        "    def __init__(self, action_size, lr=0.001, gamma=0.99, seed = 85):\n",
        "        self.gamma = gamma\n",
        "        self.ac_model = ActorCriticModel(action_size=action_size)\n",
        "        self.ac_model.compile(tf.keras.optimizers.Adam(learning_rate=lr))\n",
        "        np.random.seed(seed)\n",
        "    \n",
        "    def sample_action(self, state):\n",
        "        \"\"\"\n",
        "        Given a state, compute the policy distribution over all actions and sample one action\n",
        "        \"\"\"\n",
        "        pi,_ = self.ac_model(state)\n",
        "\n",
        "        action_probabilities = tfp.distributions.Categorical(probs=pi)\n",
        "        sample = action_probabilities.sample()\n",
        "\n",
        "        return int(sample.numpy()[0])\n",
        "\n",
        "    def actor_loss(self, action, pi, delta):\n",
        "        \"\"\"\n",
        "        Compute Actor Loss\n",
        "        \"\"\"\n",
        "        return -tf.math.log(pi[0,action]) * delta\n",
        "\n",
        "    def critic_loss(self,delta):\n",
        "        \"\"\"\n",
        "        Critic loss aims to minimize TD error\n",
        "        \"\"\"\n",
        "        return delta**2\n",
        "\n",
        "    @tf.function\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        For a given transition (s,a,s',r) update the paramters by computing the\n",
        "        gradient of the total loss\n",
        "        \"\"\"\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            pi, V_s = self.ac_model(state)\n",
        "            _, V_s_next = self.ac_model(next_state)\n",
        "\n",
        "            #V_s_next = tf.stop_gradient(V_s_next)\n",
        "\n",
        "            V_s = tf.squeeze(V_s)\n",
        "            V_s_next = tf.squeeze(V_s_next)\n",
        "            \n",
        "\n",
        "            #### TO DO: Write the equation for delta (TD error)\n",
        "            ## Write code below\n",
        "            delta = reward + self.gamma*V_s_next - V_s\n",
        "\n",
        "            loss_a = self.actor_loss(action, pi, delta)\n",
        "            loss_c =self.critic_loss(delta)\n",
        "            loss_total = loss_a + loss_c\n",
        "\n",
        "        gradient = tape.gradient(loss_total, self.ac_model.trainable_variables)\n",
        "        self.ac_model.optimizer.apply_gradients(zip(gradient, self.ac_model.trainable_variables))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUJwznIzwBIX"
      },
      "source": [
        "### Train the Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0SB0o_OfyGN",
        "outputId": "3ba85ab7-932a-4902-fefe-700ac0d0af29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode  100 Reward 141.000000 Average Reward 74.500000\n",
            "Episode  200 Reward 123.000000 Average Reward 117.200000\n",
            "Episode  300 Reward 200.000000 Average Reward 187.000000\n",
            "Stopped at Episode  276\n",
            "0:07:39.803370\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "#Initializing Agent\n",
        "agent = Agent(lr=1e-4, action_size=env.action_space.n)\n",
        "#Number of episodes\n",
        "episodes = 1800\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "reward_list = []\n",
        "average_reward_list = []\n",
        "begin_time = datetime.datetime.now()\n",
        "\n",
        "for ep in range(1, episodes + 1):\n",
        "    state = env.reset().reshape(1,-1)\n",
        "    done = False\n",
        "    ep_rew = 0\n",
        "    while not done:\n",
        "        action = agent.sample_action(state) ##Sample Action\n",
        "        next_state, reward, done, info = env.step(action) ##Take action\n",
        "        next_state = next_state.reshape(1,-1)\n",
        "        ep_rew += reward  ##Updating episode reward\n",
        "        agent.learn(state, action, reward, next_state, done) ##Update Parameters\n",
        "        state = next_state ##Updating State\n",
        "    reward_list.append(ep_rew)\n",
        "\n",
        "    if ep % 100 == 0:\n",
        "        avg_rew = np.mean(reward_list[-10:])\n",
        "        print('Episode ', ep, 'Reward %f' % ep_rew, 'Average Reward %f' % avg_rew)\n",
        "\n",
        "    if ep % 100:\n",
        "        avg_100 =  np.mean(reward_list[-100:])\n",
        "        average_reward_list.append(avg_100)\n",
        "        if avg_100 > 195.0:\n",
        "            print('Stopped at Episode ',ep-100)\n",
        "            break\n",
        "\n",
        "time_taken = datetime.datetime.now() - begin_time\n",
        "print(time_taken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twYwsX7Hu1V1"
      },
      "source": [
        "### **Task 2b**: Plot total reward curve\n",
        "In the cell below, write code to plot the total reward averaged over 100 episodes (moving average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "wOZzkLIgvHgS",
        "outputId": "0b69d6d4-6231-4a24-b0e7-0d390f3e9277"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Reward')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Plot of total reward vs episode\n",
        "## Write Code Below\n",
        "\n",
        "plt.plot(average_reward_list)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjC1dxbuuNL2"
      },
      "source": [
        "### Code for rendering ([source](https://colab.research.google.com/drive/1D6bvoEVukil7DUaJU465vtfuDgLDbOY7#scrollTo=qbIMMkfmRHyC))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2cisMBqgg9Y"
      },
      "outputs": [],
      "source": [
        "# Render an episode and save as a GIF file\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def render_episode(env: gym.Env, model: tf.keras.Model, max_steps: int): \n",
        "  screen = env.render(mode='rgb_array')\n",
        "  im = Image.fromarray(screen)\n",
        "\n",
        "  images = [im]\n",
        "  \n",
        "  state = tf.constant(env.reset(), dtype=tf.float32)\n",
        "  for i in range(1, max_steps + 1):\n",
        "    state = tf.expand_dims(state, 0)\n",
        "    action_probs, _ = model(state)\n",
        "    action = np.argmax(np.squeeze(action_probs))\n",
        "    state, _, done, _ = env.step(action)\n",
        "    state = tf.constant(state, dtype=tf.float32)\n",
        "\n",
        "    # Render screen every 10 steps\n",
        "    if i % 10 == 0:\n",
        "      screen = env.render(mode='rgb_array')\n",
        "      images.append(Image.fromarray(screen))\n",
        "  \n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return images\n",
        "\n",
        "\n",
        "# Save GIF image\n",
        "images = render_episode(env, agent.ac_model, 200)\n",
        "image_file = 'cartpole-v0.gif'\n",
        "# loop=0: loop forever, duration=1: play each frame for 1ms\n",
        "images[0].save(\n",
        "    image_file, save_all=True, append_images=images[1:], loop=0, duration=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BT7j10-9zms"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(image_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgDnDpjC_d9q"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5LBh6_lOVBdN"
      ],
      "name": "Tutorial_4_DQN_and_AC.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}